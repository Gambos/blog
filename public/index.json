
[{"content":"","date":"November 26, 2025","externalUrl":null,"permalink":"/blog/projects/","section":"","summary":"","title":"","type":"projects"},{"content":"","date":"November 26, 2025","externalUrl":null,"permalink":"/blog/tags/data-engineer/","section":"Tags","summary":"","title":"Data Engineer","type":"tags"},{"content":"","date":"November 26, 2025","externalUrl":null,"permalink":"/blog/","section":"Home Page","summary":"","title":"Home Page","type":"page"},{"content":"test test\n","date":"November 26, 2025","externalUrl":null,"permalink":"/blog/projects/project2/","section":"","summary":"","title":"My Second DE Project","type":"projects"},{"content":"","date":"November 26, 2025","externalUrl":null,"permalink":"/blog/tags/python/","section":"Tags","summary":"","title":"Python","type":"tags"},{"content":"","date":"November 26, 2025","externalUrl":null,"permalink":"/blog/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"November 21, 2025","externalUrl":null,"permalink":"/blog/tags/azure/","section":"Tags","summary":"","title":"Azure","type":"tags"},{"content":"test test\n","date":"November 21, 2025","externalUrl":null,"permalink":"/blog/projects/project1/","section":"","summary":"","title":"My First DE Project","type":"projects"},{"content":"","date":"November 20, 2025","externalUrl":null,"permalink":"/blog/posts/","section":"","summary":"","title":"","type":"posts"},{"content":"","date":"November 20, 2025","externalUrl":null,"permalink":"/blog/tags/example/","section":"Tags","summary":"","title":"Example","type":"tags"},{"content":"","date":"November 20, 2025","externalUrl":null,"permalink":"/blog/tags/hugo/","section":"Tags","summary":"","title":"Hugo","type":"tags"},{"content":"ÂïäÂïäÂïä\n","date":"November 20, 2025","externalUrl":null,"permalink":"/blog/posts/test-copy-2/","section":"","summary":"","title":"Á¥Ø","type":"posts"},{"content":"","date":"November 19, 2025","externalUrl":null,"permalink":"/blog/tags/frontend/","section":"Tags","summary":"","title":"Frontend","type":"tags"},{"content":" Bolwfish File Structure Overview article, list, taxonomy\nTarget to modify\nF12\nSearch Keywords In your IDE\nHTML element:\nDiv, Section, Span, a, p\nCSS element:\nPriority of class control: not everything form tailwind is in the theme\nResponsive vs. Scaling\nBest practice in general\n","date":"November 19, 2025","externalUrl":null,"permalink":"/blog/posts/hugo-website-custimisation/","section":"","summary":"","title":"Unlock the Full Potential of Hugo Blowfish-Themed Website","type":"posts"},{"content":"","date":"November 19, 2025","externalUrl":null,"permalink":"/blog/tags/web-development/","section":"Tags","summary":"","title":"Web Development","type":"tags"},{"content":"","date":"November 17, 2025","externalUrl":null,"permalink":"/blog/tags/sql/","section":"Tags","summary":"","title":"SQL","type":"tags"},{"content":"Âè≥Ëæπtable of contentÁöÑÊ†∑ÂºèËøòÊ≤°ÊîπÔºåË∑üÁæäÂÆ∂Â∫ôÁöÑ‰∏ëÈûã‰∏ÄÊ†∑‰∏ë\n1.\tHow to optimise SQL \u0026ndash; Use Explain plan\r#\ra)\tWhat is the Explain Plan?\r#\rAn execution plan describes how the database engine executes a SQL statement ‚Äî the path it takes through indexes, joins, and tables. It can be overridden by using a hint in query.\nb)\tHow to read and use the Explain Plan\r#\rCost: An estimated measure of resource consumption (lower is better, but this is not absolute). Access path: Whether the engine uses a full table scan, index scan, or nested loop. Join methods: Nested loop, hash join, or merge join ‚Äî each has different performance implications depending on data size and indexing. Order of operations: The execution starts from the most indented line upward, this is also reflected in the sequence number. By comparing the plan to your intended logic, you can identify inefficiencies such as unnecessary full table scans or expensive join orders. This is also an extremely useful view to help you recognise if you are on the right track when you are adjusting and optimising your query. 2.\tTypical Issues for non-optimised SQL and solutions\r#\ra) Poorly written SQL Statement\r#\rUsing SELECT * from large table Bad example üö´ Do you really need them all? Good example ‚úÖOnly take the columns needed for transformation/migration 2)\tUse ‚ÄòLEFT JOIN + WHERE condition IS NOT NULL‚Äô instead of INNER JOIN Bad example üö´ If in table departments there is no null dept_name value, Good example ‚úÖ then the query above is equivalent to more efficient INNER JOIN\nJoin same table multiple times/Create multiple very similar CTEs and join on them Bad example üö´ Good example ‚úÖ Use CASE WHEN and GROUP BY instead, try to cut down the level of aggregation:\nInefficient JOIN: not utilize different cardinality of tables Cardinality refers to the number of distinct values in a column: High cardinality: The column has many unique values, and each value appears only a few times. Examples: customer_id, order_id, email; Low cardinality: The column has only a few distinct values, and many rows share the same value. Examples: country_id, gender, status. When a low-cardinality column is used in a WHERE or JOIN condition, it has weak filtering power ‚Äî even if it has an index, the database might still scan a large portion of the table. In contrast, a high-cardinality column provides strong selectivity and allows the optimizer to quickly narrow down the relevant rows.\nWhen the database decides the join order, it prefers to process the table that can reduce the data volume as early as possible‚Äîso tables with high cardinality and high selectivity should be used as the driving tables, and tables with low cardinality (such as country or status) should be joined later, because they tend to produce large intermediate result sets.\nThat‚Äôs why we need to keep in mind that the optimizer might assume that starting the hash join from the small table country is cheaper (because the table itself is small), but it overlooks the fact that its low cardinality causes the query to scan the entire orders table‚Äîthat‚Äôs why we need to check explain plan to see if it chooses full scan in this case.\nBad example üö´ If table country only has a few distinct values (low cardinality) and without any filtering power, the join may start the join from table country and produces huge intermediate sets and forces full scan of table order.\nGood example ‚úÖ If possible, apply a filter, and the optimizer can estimate the selectivity more accurately, as it realizes that only part of orders belongs to ‚ÄòUS‚Äô, and it can use the index (if added) on o.country_id to limit the scan of large table order.\nOveruse DISTINCT Bad example üö´ DISTINCT masks duplicate logic errors from bad join, but the engine still processes all rows before deduplication ‚Äî heavy sort or hash step. Good example ‚úÖ Identify and remove duplication at the source (e.g., incorrect joins or UNIONs) rather than applying DISTINCT as a patch.\nComplex subquery Bad example üö´ Inefficient nested subqueries Good example ‚úÖ Fix by using joins or CTE\nUsing NOT IN instead of NOT EXISTS Bad example üö´ If the subquery returns 1 NULL, the entire where condition fails (if one value ever equals to NULL? SQL considers it as unknown), so no row returned for the whole query. Good example ‚úÖ In EXISTS, each comparison is evaluated row by row using the = operator. Since NULL is not equal to any value (including another NULL), such rows never satisfy the match condition, so the result will not be disrupted by NULL value.\nExample matrix for understanding (if employees.dept_id = 30): Subquery dept_id values\tNOT IN result\tNOT EXISTS result {10, NULL}\tfiltered out\treturned {10, 20}\treturned\treturned\nb)\tIndex Missing or Inefficient Usage\r#\rIndexes are one of the most important tools to improve SQL performance. They allow the database to locate rows quickly without scanning the entire table. Issue\tExplain plan Key Phrase\tWhy it‚Äôs bad\tHow to Fix it\tBefore Example\tAfter Example Completely Missing Index\tTABLE ACCESS FULL\tFull table scan instead of index scan will slow down the performance.\tCreate index on columns which used in:\nWhere Join SELECT * FROM employees WHERE emp_id = 123;\tCREATE INDEX emp_id_idx ON employees(emp_id); SELECT * FROM employees WHERE emp_id = 123; Composite Index Missing\tTABLE ACCESS FULL; INDEX RANGE SCAN on partial column\tResult in suboptimal query performance as it does not leverage multiple single-column indexes effectively.\tCreate a composite (multi-column) index matching the filter or join condition order.\tCREATE INDEX orders_cust_date_idx ON orders(customer_id)\nSELECT * FROM orders WHERE customer_id = 10 AND order_date = DATE \u0026lsquo;2025-01-01\u0026rsquo;;\tCREATE INDEX orders_cust_date_idx ON orders(customer_id, order_date);\nSELECT * FROM orders WHERE customer_id = 10 AND order_date = DATE \u0026lsquo;2025-01-01\u0026rsquo;; Function Use on Indexed columns\tTABLE ACCESS FULL; FUNCTION in filter predicate\tUsing functions on indexed columns prevents the use of indexes.\tUse a function-based index\tCREATE INDEX emp_lastname_upper_idx ON employees(last_name);\nSELECT * FROM employees WHERE UPPER(last_name) = \u0026lsquo;SMITH\u0026rsquo;;\tCREATE INDEX emp_lastname_upper_idx ON employees(UPPER(last_name));\nSELECT * FROM employees WHERE UPPER(last_name) = \u0026lsquo;SMITH\u0026rsquo;;\nHow Index Works and Why It Speeds Up Queries? An index is essentially a sorted structure that maps key column values to physical row locations. When a query filters or joins on an indexed column, the database can: ‚Ä¢\tUse the index to quickly locate relevant rows (INDEX RANGE SCAN) ‚Ä¢\tAvoid scanning every row in the table (FULL TABLE SCAN) This reduces I/O and CPU usage, especially on large tables, because only the subset of relevant rows is read.\nGuideline for index setup: ‚Ä¢\tColumns in WHERE and JOIN condition clauses ‚Üí high-priority index candidates ‚Ä¢\tHighly selective columns first in composite indexes\nHow to know if the newly added index helps or not? Check the execution plan: ‚Ä¢\tINDEX RANGE SCAN or INDEX UNIQUE SCAN ‚Üí index is used ‚Ä¢\tFULL TABLE SCAN ‚Üí index is ignored Other indicators: ‚Ä¢\tQuery runtime/cost decreases after adding the index ‚Ä¢\tNumber of logical reads drops Notice that an index is not automatically helpful if: ‚Ä¢\tQueries wrap the indexed column in a function (WHERE UPPER(indexed_column) = \u0026lsquo;ALICE\u0026rsquo;) ‚Ä¢\tColumn has very low cardinality (e.g., a boolean column which only has T and F two values)\nShall I use Single vs. Composite Index? ‚Ä¢\tSingle-column index: best for queries filtering or sorting on one column. ‚Ä¢\tComposite index (multi-column): effective when queries filter on multiple columns together. o\tExample: WHERE a = ? AND b = ? ‚Üí composite index on (a, b) is more efficient than two separate indexes. o\tOrder matters: the first column should be with the highest cardinality or most commonly filtered, it would be the ideal column to create index on.\nINDEX SCAN variants and how to optimise? Execution plans can show different scan types: ‚Ä¢\tINDEX UNIQUE SCAN: single-row lookup using a unique or primary key index. ‚Ä¢\tINDEX RANGE SCAN: range search on an ordered index. ‚Ä¢\tINDEX FULL SCAN: scanning the entire index; sometimes faster than a table scan if the index is smaller. ‚Ä¢\tINDEX FAST FULL SCAN: index-only scan for queries that do not require table columns.\nWhat to do if the execution plan ignores a new index? ‚Ä¢\tCheck statistics: stale statistics can mislead the optimizer EXEC DBMS_STATS.GATHER_TABLE_STATS(\u0026lsquo;schema\u0026rsquo;,\u0026rsquo;table\u0026rsquo;); EXEC DBMS_STATS.GATHER_INDEX_STATS(\u0026lsquo;schema\u0026rsquo;,\u0026lsquo;index_name\u0026rsquo;); ‚Ä¢\tCheck query expressions: functions, implicit conversions, or mismatched data types may prevent index usage. ‚Ä¢\tReview selectivity: low-selectivity columns may not be used; consider filtering more selective columns first. ‚Ä¢\tUse index hints cautiously: Use SELECT /*+ INDEX(table index_name) */ \u0026hellip; Only if the optimizer consistently ignores a useful index. ‚Ä¢\tCheck plan after changes: always verify the execution plan after adding or modifying indexes to ensure they are actually being used.\nc)\tHints‚ÄîLast Resort\r#\rSQL hints are directives embedded in a query that influence the optimizer‚Äôs execution plan. They don‚Äôt change the query result but can force specific behaviour and overwrite original explain plan when the optimizer‚Äôs default choice isn‚Äôt optimal.\nHow does hint work? Normally, the optimizer determines the most efficient execution plan based on available statistics and cost estimations. However, when statistics are incomplete, or the data distribution is skewed, the optimizer may choose a suboptimal path. Hints explicitly instruct the optimizer to use a specific plan component (e.g., index, parallelism etc.). Hints apply only to the statement where they appear, and their effect is visible in the EXPLAIN PLAN. Index Hints\nThis tells the optimizer to use a specific index emp_dept_idx on the table e. It is useful when the optimizer ignores an available index because of outdated statistics or complex conditions. If emp_dept_idx is an index on column department_id, this hint forces an index range scan instead of a full table scan.\nParallelism Hints Parallelism hints help because they tell the database optimizer to divide a large workload into smaller chunks and execute them simultaneously across multiple CPU cores or processes. It can significantly speed up large analytical or batch operations in this way. Table-level parallelism:\nPARALLEL(4) means using 4 parallel execution servers, it requests 4 parallel execution servers for the employees table Each process scans a portion of the table concurrently, and results are merged. Works best for large table scans and aggregation-heavy queries. Statement-level parallelism:\nThis hint applies parallelism to the entire SQL operation, not just one table. Parallel Index Scan:\nThis hint forces parallel index range scan using 8 parallel processes on index sales_idx.\nWhen shall we use hint? Hints should be a last resort, not a first choice. They are useful when statistics are correct but the optimizer still makes a poor choice. Avoid overusing hints‚Äîthey make queries less adaptable to schema or data changes. DO NOT use PARALLEL on small tables‚Äîit can cause contention and excessive overhead.\nd) Incomplete Statistics\r#\rEven after you have fixed all the poor query, missing index etc., the explain plan still won‚Äôt show the improvement. This could be because the stale optimizer statistics of table could lead to faulty information and generate suboptimal execution plans.\nMost common cause ‚Ä¢\tTables or indexes were recently updated (INSERT/UPDATE/DELETE) but not analyzed.\n‚Ä¢\t‚ÄòDBMS_STATS‚Äô has not been run for a long time.\nSolutions ‚Ä¢\tGather fresh statistics for affected tables and indexes\n‚Ä¢\tCheck table for missing statistics\n","date":"November 17, 2025","externalUrl":null,"permalink":"/blog/posts/sql-optimisation-guidance/","section":"","summary":"","title":"SQL Optimisation Cheetsheet","type":"posts"},{"content":"\r","externalUrl":null,"permalink":"/blog/illustration/","section":"Home Page","summary":"","title":"","type":"page"},{"content":"","externalUrl":null,"permalink":"/blog/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/blog/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"THIS IS MY CV\n","externalUrl":null,"permalink":"/blog/cv/","section":"Home Page","summary":"","title":"CV","type":"page"},{"content":"","externalUrl":null,"permalink":"/blog/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","externalUrl":null,"permalink":"/blog/tags/web-build/","section":"Tags","summary":"","title":"Web Build","type":"tags"}]